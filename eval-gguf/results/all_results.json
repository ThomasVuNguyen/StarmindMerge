{
  "metadata": {
    "timestamp": "2025-09-08T20:35:44.658297",
    "total_models": 6,
    "version": "1.0"
  },
  "models": {
    "gemma-3-270m-it-q8_0": {
      "model_path": "gguf/gemma-3-270m-it-q8_0.gguf",
      "model_name": "gemma-3-270m-it-q8_0",
      "tasks": {
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 70,
          "score_numeric": 29.3928
        },
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 20,
          "score_numeric": 53.6701
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 9,
          "score_numeric": 27.7592
        }
      }
    },
    "gemma-3-270m-it-bf16": {
      "model_path": "gguf/gemma-3-270m-it-bf16.gguf",
      "model_name": "gemma-3-270m-it-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-code-bf16": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-code-bf16.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-code-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0",
      "tasks": {
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 21,
          "score_numeric": 53.6701
        },
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 71,
          "score_numeric": 29.3928
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 9,
          "score_numeric": 27.7592
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-code-q8_0": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-code-q8_0.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-code-q8_0",
      "tasks": {
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 20,
          "score_numeric": 53.6701
        },
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 71,
          "score_numeric": 29.3928
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 38,
          "score_numeric": 27.7592
        }
      }
    }
  }
}