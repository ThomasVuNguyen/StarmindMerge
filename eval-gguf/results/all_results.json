{
  "metadata": {
    "timestamp": "2025-09-09T12:02:07.153125",
    "total_models": 8,
    "version": "1.0"
  },
  "models": {
    "gemma-3-270m-it-q8_0": {
      "model_path": "gguf/gemma-3-270m-it-q8_0.gguf",
      "model_name": "gemma-3-270m-it-q8_0",
      "tasks": {
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 70,
          "score_numeric": 29.3928
        },
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 20,
          "score_numeric": 53.6701
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 9,
          "score_numeric": 27.7592
        }
      }
    },
    "gemma-3-270m-it-bf16": {
      "model_path": "gguf/gemma-3-270m-it-bf16.gguf",
      "model_name": "gemma-3-270m-it-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-instruction-following-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-code-bf16": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-code-bf16.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-code-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "38.25000000%\t[33.6205%, 43.1031%]",
          "execution_time_seconds": 65,
          "score_numeric": 38.25
        },
        "Winogrande": {
          "score": "54.1437 +/- 1.4004",
          "execution_time_seconds": 39,
          "score_numeric": 54.1437
        },
        "MMLU": {
          "score": "29.2636 +/- 1.1568",
          "execution_time_seconds": 130,
          "score_numeric": 29.2636
        },
        "TruthfulQA": {
          "score": "24.8470 +/- 1.5127",
          "execution_time_seconds": 54,
          "score_numeric": 24.847
        },
        "ARC-Combined": {
          "score": "40.0460 +/- 1.6631",
          "execution_time_seconds": 37,
          "score_numeric": 40.046
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-instruction-following-q8_0",
      "tasks": {
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 21,
          "score_numeric": 53.6701
        },
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 71,
          "score_numeric": 29.3928
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 9,
          "score_numeric": 27.7592
        }
      }
    },
    "gemma-3-270m-tulu-3-sft-personas-code-q8_0": {
      "model_path": "gguf/gemma-3-270m-tulu-3-sft-personas-code-q8_0.gguf",
      "model_name": "gemma-3-270m-tulu-3-sft-personas-code-q8_0",
      "tasks": {
        "Hellaswag": {
          "score": "38.00000000%\t[33.3787%, 42.8495%]",
          "execution_time_seconds": 42,
          "score_numeric": 38.0
        },
        "Winogrande": {
          "score": "53.6701 +/- 1.4015",
          "execution_time_seconds": 20,
          "score_numeric": 53.6701
        },
        "MMLU": {
          "score": "29.3928 +/- 1.1582",
          "execution_time_seconds": 71,
          "score_numeric": 29.3928
        },
        "TruthfulQA": {
          "score": "25.0918 +/- 1.5177",
          "execution_time_seconds": 35,
          "score_numeric": 25.0918
        },
        "ARC-Combined": {
          "score": "41.1968 +/- 1.6706",
          "execution_time_seconds": 21,
          "score_numeric": 41.1968
        },
        "ARC-Challenge": {
          "score": "27.7592 +/- 2.5941",
          "execution_time_seconds": 38,
          "score_numeric": 27.7592
        }
      }
    },
    "gm3-270m-math-bf16": {
      "model_path": "gguf/gm3-270m-math-bf16.gguf",
      "model_name": "gm3-270m-math-bf16",
      "tasks": {
        "Hellaswag": {
          "score": "34.75000000%\t[30.2486%, 39.5415%]",
          "execution_time_seconds": 62,
          "score_numeric": 34.75
        },
        "Winogrande": {
          "score": "54.9329 +/- 1.3984",
          "execution_time_seconds": 38,
          "score_numeric": 54.9329
        },
        "MMLU": {
          "score": "28.4884 +/- 1.1476",
          "execution_time_seconds": 126,
          "score_numeric": 28.4884
        },
        "TruthfulQA": {
          "score": "24.2350 +/- 1.5001",
          "execution_time_seconds": 52,
          "score_numeric": 24.235
        },
        "ARC-Combined": {
          "score": "39.4707 +/- 1.6591",
          "execution_time_seconds": 36,
          "score_numeric": 39.4707
        },
        "ARC-Challenge": {
          "score": "26.4214 +/- 2.5541",
          "execution_time_seconds": 15,
          "score_numeric": 26.4214
        }
      }
    },
    "gm3-270m-math-q8_0": {
      "model_path": "gguf/gm3-270m-math-q8_0.gguf",
      "model_name": "gm3-270m-math-q8_0",
      "tasks": {
        "Hellaswag": {
          "score": "35.25000000%\t[30.7287%, 40.0519%]",
          "execution_time_seconds": 40,
          "score_numeric": 35.25
        },
        "Winogrande": {
          "score": "54.2226 +/- 1.4002",
          "execution_time_seconds": 20,
          "score_numeric": 54.2226
        },
        "MMLU": {
          "score": "28.6822 +/- 1.1499",
          "execution_time_seconds": 69,
          "score_numeric": 28.6822
        },
        "TruthfulQA": {
          "score": "24.4798 +/- 1.5052",
          "execution_time_seconds": 34,
          "score_numeric": 24.4798
        },
        "ARC-Combined": {
          "score": "39.9310 +/- 1.6623",
          "execution_time_seconds": 20,
          "score_numeric": 39.931
        },
        "ARC-Challenge": {
          "score": "25.0836 +/- 2.5112",
          "execution_time_seconds": 9,
          "score_numeric": 25.0836
        }
      }
    }
  }
}